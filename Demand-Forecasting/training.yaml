apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: demand-forecasting-training # Executable ID (max length 64 lowercase-hyphen-separated), please modify this to any value if you are not the only user of your SAP AI Core instance. Example: `first-pipeline-1234`
  annotations:
    scenarios.ai.sap.com/description: "Training for Demand Forecasting"
    scenarios.ai.sap.com/name: "Training"
    executables.ai.sap.com/description: "Train the demand forecasting model"
    executables.ai.sap.com/name: "demand forecasting training"
    artifacts.ai.sap.com/train-dataset.kind: "dataset" # Helps in suggesting the kind of artifact that can be attached.
    artifacts.ai.sap.com/forecastingmodel.kind: "model" # Helps in suggesting the kind of artifact that can be generated.
  labels:
    scenarios.ai.sap.com/id: "Training"
    ai.sap.com/version: "2.0"
spec:
  imagePullSecrets:
    - name: mm-credentials-docker # your docker registry secret
  entrypoint: mypipeline
  templates:
  - name: mypipeline
    steps:
    - - name: training
        template: mycodeblock1
  - name: mycodeblock1
    inputs:
      artifacts:  # placeholder for cloud storage attachements
        - name: trainingdata # a name for the placeholder
          path: /app/data/ # where to copy in the Dataset in the Docker image
    outputs:
      artifacts:
        - name: forecast # name of the artifact generated, and folder name when placed in S3, complete directory will be `../<executaion_id>/housepricemodel`
          globalName: getforecast# local identifier name to the workflow, also used above in annotation
          path: /app/model/ # from which folder in docker image (after running workflow step) copy contents to cloud storage
          archive:
            none:   # specify not to compress while uploading to cloud
              {}
    container:
      image: docker.io/moermansm/demand-forecasting:01
      command: ["/bin/sh", "-c"]
      args:
        - "python /app/src/main.py"
